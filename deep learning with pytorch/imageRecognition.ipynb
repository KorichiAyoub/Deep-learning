{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "lst = [[1,2,3], [4,5,6]]\n",
    "tensor = torch.tensor(lst)\n",
    "\n",
    "#from numpy array\n",
    "np_array = np.array([1,2,4])\n",
    "np_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "tensor.dtype\n",
    "tensor.device\n",
    "\n",
    "# can add mul dev sub tensors ... most np operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3135, 0.2080]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "input_tensor = torch.tensor([[0.3471,0.4547,-0.2356]])\n",
    "\n",
    "#linear layer\n",
    "linear_layer= nn.Linear(in_features=3,out_features= 2)\n",
    "\n",
    "# return output \n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0137, -0.4947, -0.3276],\n",
       "        [-0.3256,  0.3406, -0.3048]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4565, 0.0944], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3471,  0.4547, -0.2356]])\n"
     ]
    }
   ],
   "source": [
    "#regression model for classification we add sigmoid or softmax\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18),\n",
    "    nn.Linear(18,20),\n",
    "    nn.Linear(20,5)\n",
    ")\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1185, 0.8756, 0.0059]])\n"
     ]
    }
   ],
   "source": [
    "# now we add activation function to add onon linearity\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)# used as last step when using binary classification\n",
    "\n",
    "# softmax used for multi class classifiaction takes n elem vect and output same size\n",
    "input_tensor = torch.tensor([[4,6.0,1]])\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass to predict and give output\n",
    "# backward used for updating weights \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding \n",
    "import torch.nn.functional as F\n",
    "F.one_hot(torch.tensor(2),num_classes = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import tensor\n",
    "\n",
    "scores = tensor([[-0.1211,0.1059]])# the values of the model before activation function\n",
    "one_hot_target = tensor([[1,0]])# the onehot encoding of the right class\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(),one_hot_target.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.8745e-03, -2.9625e-03,  4.6295e-03,  8.2846e-04,  6.0182e-03,\n",
      "        -4.7606e-03,  1.4616e-03,  4.1604e-03,  2.4318e-03, -2.5598e-04,\n",
      "         2.0931e-03, -1.5537e-03, -8.0914e-04,  2.5300e-03,  3.4674e-03,\n",
      "        -1.6253e-03,  2.1685e-03, -7.4808e-04,  4.2180e-03, -3.1880e-03,\n",
      "        -4.4371e-03,  1.5576e-03, -1.7497e-03,  2.4522e-03,  4.7469e-03,\n",
      "        -5.5151e-06,  1.7311e-03, -3.2607e-04, -1.2925e-03,  9.1881e-04,\n",
      "         6.2203e-04, -1.0011e-03, -7.0800e-04,  1.6719e-03,  3.4598e-04,\n",
      "         1.1780e-03, -1.3271e-03, -1.0800e-03,  5.2166e-04,  1.3043e-03,\n",
      "        -2.9848e-03,  2.9658e-04, -5.0549e-03, -8.4453e-03, -9.7727e-03,\n",
      "         2.0801e-03, -8.0685e-03, -2.3645e-04, -4.8065e-03,  4.2095e-03,\n",
      "         4.4672e-03, -1.9033e-03, -3.2240e-03,  8.0816e-03, -4.4857e-03,\n",
      "         1.7254e-03,  1.4479e-03, -8.0130e-03,  2.9207e-03,  2.8712e-03,\n",
      "        -2.0194e-03, -4.2629e-04, -3.9377e-03, -1.5123e-03, -2.2006e-03,\n",
      "         4.6726e-03, -3.1863e-03,  8.6314e-04, -2.9169e-03, -3.3820e-03,\n",
      "        -4.3286e-04,  1.2668e-03,  4.4004e-03,  8.5880e-04, -1.0433e-03,\n",
      "        -3.8595e-03, -5.1809e-04,  2.5721e-03, -3.8132e-04,  1.9363e-03,\n",
      "         1.6061e-03, -2.0120e-03, -1.0917e-03, -1.7871e-03, -5.7108e-03,\n",
      "         6.4154e-03, -2.0693e-03, -3.2371e-03, -2.5634e-03,  2.3457e-03,\n",
      "         5.8429e-03,  2.0100e-03, -4.0878e-03,  4.9877e-04,  2.2602e-03,\n",
      "        -5.5408e-03,  4.4295e-03, -2.0963e-04,  3.5900e-03,  6.7233e-04,\n",
      "        -5.6438e-04, -4.7180e-04, -1.3213e-03, -7.4926e-03, -3.2529e-03,\n",
      "        -4.1479e-03, -4.6904e-04, -8.5732e-04,  6.7541e-03, -1.7521e-03,\n",
      "        -2.3007e-03, -3.1945e-03,  3.3195e-03, -4.1273e-03,  2.1743e-03,\n",
      "         2.6198e-03, -1.1852e-03,  3.5282e-03, -2.6493e-03, -3.1692e-03,\n",
      "        -1.9558e-04,  2.2661e-03,  6.4536e-03, -2.4500e-03, -3.3455e-03,\n",
      "        -2.6858e-03, -3.8601e-03,  1.0419e-03, -3.1341e-03, -1.8360e-03,\n",
      "        -4.1946e-03,  1.0958e-03,  9.5527e-03, -7.3637e-03,  6.0806e-04,\n",
      "         6.6193e-03, -1.1134e-02, -9.5372e-04, -9.3319e-03, -5.2962e-03,\n",
      "         2.3476e-03,  1.5223e-03,  1.5777e-03,  4.0998e-04,  1.3142e-03,\n",
      "         1.4647e-03,  7.1769e-04, -1.5580e-03,  1.6584e-04,  6.7995e-04,\n",
      "        -9.7181e-04, -3.7088e-03, -3.3377e-03,  1.9134e-03,  4.1646e-03,\n",
      "        -5.3258e-03,  6.2677e-03, -1.2828e-03,  1.4799e-03, -1.1896e-03,\n",
      "        -1.8232e-03,  6.3112e-04,  4.8187e-03, -2.6263e-03, -4.4349e-03,\n",
      "         5.3618e-03, -2.1300e-03, -2.4054e-03, -2.6577e-03, -3.2308e-03,\n",
      "        -1.0085e-03, -5.0585e-04, -9.9391e-04, -7.7534e-04,  4.5841e-03,\n",
      "        -3.2322e-04, -1.7715e-03, -1.7711e-03,  7.4602e-05, -3.8487e-04,\n",
      "        -2.2666e-03,  3.3660e-04,  7.3195e-04,  9.7561e-04,  2.8048e-04,\n",
      "         2.4061e-03,  9.3242e-04,  6.2010e-04, -1.8918e-03,  3.7072e-03,\n",
      "         4.1500e-03, -1.0966e-03, -9.0924e-04, -7.8452e-03, -1.7389e-03,\n",
      "        -3.8210e-03, -4.3924e-03, -1.0134e-03, -1.1231e-03,  1.5159e-03,\n",
      "         8.7400e-05,  2.9046e-03, -1.7019e-03,  2.9365e-03,  1.0094e-03,\n",
      "         1.2534e-04, -2.2195e-03, -3.6732e-03,  6.9698e-04,  1.1040e-03,\n",
      "        -1.2589e-03, -2.9942e-03, -1.1096e-04,  2.1974e-03,  8.3373e-04,\n",
      "        -2.3478e-03, -1.2869e-03,  2.8236e-03, -1.3925e-04,  9.3160e-04,\n",
      "         5.6251e-04,  1.9423e-03,  5.7796e-04, -4.4782e-04, -3.4809e-04,\n",
      "         7.8798e-04,  7.1064e-04,  2.4656e-04,  1.9464e-03, -9.5250e-04,\n",
      "         2.8913e-04,  2.4745e-03,  3.6925e-04,  1.9798e-04, -5.7114e-03,\n",
      "         7.2092e-04, -8.2533e-03, -4.3486e-03, -5.9724e-03, -2.8633e-03,\n",
      "        -4.0714e-03, -6.6944e-04, -3.5779e-03, -1.2031e-02, -6.9529e-03,\n",
      "        -3.7265e-03, -1.1122e-02, -4.5119e-03, -4.3787e-03, -3.2600e-03,\n",
      "        -6.9443e-03, -6.5705e-03, -7.3881e-03,  2.4156e-03, -1.8054e-04,\n",
      "        -8.7699e-04, -6.3684e-03, -6.6659e-03,  5.7185e-03,  6.9380e-04,\n",
      "        -3.3045e-03, -1.2407e-03,  2.7921e-03, -5.0360e-03, -1.1592e-02,\n",
      "        -1.2455e-02, -1.1377e-05, -3.0298e-03, -8.4221e-03, -3.4701e-03,\n",
      "        -3.0424e-04,  2.2708e-03,  5.1170e-04, -5.9495e-04, -8.5460e-04,\n",
      "        -1.9257e-05,  1.8906e-03,  7.3379e-04, -4.7065e-05,  2.0691e-03,\n",
      "         1.6803e-03,  2.8637e-04,  1.9069e-04, -5.0877e-04,  5.9227e-04,\n",
      "         3.1890e-04,  7.2791e-04,  7.0740e-04, -1.6385e-03, -3.3524e-03,\n",
      "         6.6632e-04,  3.6102e-04, -2.2724e-04, -5.5556e-03,  5.7945e-04,\n",
      "        -7.1357e-04, -1.2147e-03,  6.6275e-03,  4.3149e-03, -1.3932e-03,\n",
      "        -5.6971e-03, -3.5618e-03, -1.7855e-03, -4.2884e-03, -1.0899e-03,\n",
      "         1.6100e-03,  2.8310e-03,  3.2783e-03,  4.4859e-03,  7.4405e-04,\n",
      "         2.4033e-03,  2.7209e-03,  1.1491e-02,  6.2888e-03,  4.4130e-04,\n",
      "         1.1403e-02,  1.3880e-02,  0.0000e+00,  8.0646e-03, -5.1776e-04,\n",
      "         9.9215e-04,  2.7617e-03,  6.7634e-05,  2.4348e-03, -3.7558e-03,\n",
      "         6.7796e-06, -1.5631e-03, -2.3378e-03, -1.5835e-03, -1.5760e-03,\n",
      "        -7.7359e-03, -1.3173e-03,  1.4739e-03, -6.8166e-03, -4.4100e-03,\n",
      "        -7.2778e-04, -3.2159e-03,  4.0492e-04, -3.6921e-04, -2.7998e-03,\n",
      "         6.3654e-05, -1.7064e-03, -2.6336e-04, -1.8003e-03, -5.5846e-04,\n",
      "         3.2377e-04, -7.5396e-04, -3.1542e-04,  7.6840e-04, -3.9486e-04,\n",
      "        -1.3597e-03, -3.4650e-04, -5.5203e-04, -1.1417e-03, -6.3729e-04,\n",
      "        -6.3848e-05,  7.6164e-05, -1.1223e-04, -1.6047e-03,  0.0000e+00,\n",
      "        -4.1357e-03, -3.3126e-04, -1.2420e-03, -3.2536e-04, -4.5047e-03,\n",
      "        -1.9355e-03,  6.4549e-04,  1.1445e-03, -1.6313e-03, -1.6304e-03,\n",
      "        -6.6266e-04, -3.5078e-03, -5.4966e-03, -6.6507e-03, -1.4108e-04,\n",
      "        -9.5637e-04, -5.0240e-03, -1.3593e-03,  5.7384e-05,  3.0152e-03,\n",
      "        -2.4989e-03,  8.5947e-04, -5.4121e-03, -3.3164e-04,  3.6397e-03,\n",
      "         2.1040e-03, -3.1015e-04, -2.5518e-03, -1.0044e-03, -1.9293e-03,\n",
      "        -5.4834e-03, -6.4738e-03, -7.6195e-04,  2.0842e-04, -2.8574e-03,\n",
      "        -2.2348e-03, -1.1393e-04,  5.9726e-05,  1.5723e-04, -6.8218e-05,\n",
      "         5.8514e-04,  6.7393e-04, -3.0253e-04, -5.4521e-04,  5.4870e-04,\n",
      "        -3.5433e-04, -3.9902e-04,  1.2140e-03,  1.0236e-03,  8.8585e-04,\n",
      "        -5.9051e-04, -3.9133e-04,  8.2846e-04,  1.1056e-04, -1.7067e-03,\n",
      "         2.1840e-03,  1.0267e-03,  8.9581e-04, -1.0806e-03,  2.8769e-03,\n",
      "         1.2549e-03,  2.4389e-05, -2.9910e-03,  2.7017e-04,  2.0838e-03,\n",
      "        -1.5227e-03,  5.5168e-05, -5.0621e-03,  4.9010e-04,  3.7525e-03,\n",
      "        -3.5768e-03, -6.0877e-04,  1.0030e-04,  2.3858e-04, -4.9326e-04,\n",
      "        -2.0535e-03,  2.7459e-04, -9.8062e-04,  6.5721e-04, -2.3524e-04,\n",
      "        -9.4120e-05,  0.0000e+00, -4.6414e-04, -1.2050e-03, -4.7032e-04,\n",
      "        -7.7733e-04, -1.1644e-05,  0.0000e+00, -3.0284e-03,  0.0000e+00,\n",
      "        -1.4365e-03,  6.0519e-04, -7.5897e-04,  2.0923e-03, -3.5086e-04,\n",
      "         3.0647e-03,  1.8937e-04,  5.8736e-05, -8.0156e-04, -2.0979e-04,\n",
      "         3.8628e-04,  1.5888e-03,  9.3403e-04,  7.8440e-04,  0.0000e+00,\n",
      "         1.4189e-03,  2.8676e-03, -7.8444e-04,  1.5994e-03, -6.7981e-04,\n",
      "         3.8555e-04,  7.2975e-04,  1.7713e-03,  1.3820e-03,  6.1344e-04,\n",
      "         1.1613e-04,  1.5230e-03,  5.0768e-04,  1.0527e-03,  1.9226e-03,\n",
      "         4.3992e-03,  4.5438e-03,  3.7960e-04,  1.7471e-03,  3.5631e-03,\n",
      "         2.6315e-04,  6.1894e-04,  1.6928e-05,  0.0000e+00,  0.0000e+00,\n",
      "         3.2370e-03,  2.8773e-04,  3.2457e-04,  0.0000e+00,  1.2026e-04,\n",
      "         1.2291e-03,  3.9292e-04,  4.3044e-04,  4.7247e-03,  2.2880e-03,\n",
      "         1.9330e-05,  3.2549e-04,  6.6709e-04,  6.7984e-04, -1.0135e-03,\n",
      "        -1.3900e-04,  1.1474e-03,  2.2081e-04, -7.1336e-05,  2.4666e-04,\n",
      "         7.5039e-04, -1.6836e-04,  3.4929e-04,  1.5460e-04,  3.4188e-03,\n",
      "        -7.6185e-06,  6.5144e-04, -1.2884e-03, -5.1421e-04, -1.3115e-03,\n",
      "         1.4499e-04,  3.0105e-05, -1.9731e-03, -7.2185e-04,  3.5864e-03,\n",
      "         3.9459e-03,  1.7614e-05, -1.6721e-03,  5.1812e-03, -2.3254e-03,\n",
      "        -2.1697e-03,  4.2873e-03,  5.5987e-03,  1.3973e-04, -2.7840e-03,\n",
      "        -2.8487e-03,  2.9049e-03,  1.4040e-03,  5.3172e-04, -1.6053e-04,\n",
      "        -5.7059e-04, -2.9351e-03, -2.6069e-03, -7.1398e-04,  1.3579e-03,\n",
      "        -3.0634e-04, -1.3496e-03, -6.7040e-04,  2.3728e-03,  1.7369e-04,\n",
      "        -4.4427e-03, -2.0675e-04,  5.7365e-03,  5.9586e-03, -4.4978e-04,\n",
      "        -6.8221e-04,  2.1344e-03,  1.5386e-03,  1.5932e-03,  2.5734e-03,\n",
      "        -2.0657e-02, -1.5595e-02,  5.3326e-04, -3.7225e-03,  1.4394e-02,\n",
      "        -1.1712e-02, -4.4572e-04, -9.5439e-03, -4.8110e-03,  1.5842e-03,\n",
      "         2.6292e-03, -1.4209e-03,  2.6258e-03,  7.2230e-03,  3.5456e-03,\n",
      "         2.9749e-03,  5.0659e-03,  2.8697e-03,  1.2093e-02,  6.9384e-03,\n",
      "        -2.6791e-03,  1.2407e-02,  5.8563e-03,  5.2352e-03,  6.9684e-03,\n",
      "         1.1087e-02,  6.0312e-04, -2.3972e-04,  5.0177e-03,  1.1141e-03,\n",
      "         4.0754e-03,  6.2158e-04,  1.9169e-03,  2.4592e-03,  9.8831e-04,\n",
      "         2.4157e-04,  3.5196e-03,  5.2385e-03,  9.3589e-03,  4.8565e-03,\n",
      "         7.7281e-03,  4.9307e-03,  2.3999e-03,  3.1305e-03, -3.7391e-03,\n",
      "        -9.3551e-03, -4.2224e-04,  3.3097e-03,  7.8094e-03,  1.3566e-03,\n",
      "        -1.9251e-03,  5.9689e-04,  1.8405e-03,  9.3998e-04,  6.7314e-04,\n",
      "        -8.1852e-05,  7.7964e-03,  9.7351e-04, -2.1526e-02, -1.2434e-04,\n",
      "        -1.4962e-02, -3.1894e-02, -7.6257e-03, -3.5041e-03,  2.6060e-03,\n",
      "        -1.4568e-04,  1.0699e-03, -1.0419e-02, -1.0092e-02, -4.2447e-03,\n",
      "        -6.4943e-03, -2.7964e-03, -5.6207e-03, -3.5457e-03, -2.7696e-03,\n",
      "        -3.2018e-03, -3.5108e-03, -4.7927e-03,  2.4367e-03, -1.0213e-02,\n",
      "         7.7898e-03,  1.4049e-02, -2.0830e-03, -2.1609e-03, -1.1255e-02,\n",
      "        -6.6578e-03, -2.1016e-03,  4.3876e-03, -3.1267e-03,  2.9369e-04,\n",
      "         4.9220e-04,  8.4330e-04,  9.3599e-04,  2.2267e-03,  8.2726e-04,\n",
      "         1.5960e-03, -6.9675e-03, -2.1118e-03, -2.3624e-03, -1.4576e-03,\n",
      "         2.1229e-03,  5.0791e-04,  1.4525e-03, -2.7008e-03,  5.4195e-03,\n",
      "         5.0712e-03,  8.5083e-04,  2.9615e-03,  3.9178e-04,  1.4803e-03,\n",
      "         3.8518e-03,  7.3461e-04,  9.2733e-04, -2.0801e-03,  2.8089e-04,\n",
      "         1.4461e-03, -8.3771e-04,  6.9256e-04,  6.2231e-02,  3.5619e-02,\n",
      "        -6.2786e-02, -2.5877e-02, -9.1868e-03])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 18),\n",
    "    nn.ReLU(),  # Adding non-linearity\n",
    "    nn.Linear(18, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "# Define input data and targets\n",
    "input_data = torch.randn(100, 10)  # Example: 100 samples, 10 features\n",
    "targets = torch.randint(0, 5, (100,))  # Example: 100 targets, 5 classes\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_data)\n",
    "\n",
    "# Compute loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Extract gradients\n",
    "gradients = []\n",
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        gradients.append(param.grad.view(-1))\n",
    "gradients = torch.cat(gradients)\n",
    "\n",
    "# Print or use gradients\n",
    "print((gradients))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first linear layer (`nn.Linear(10, 18)`) has \\(10 \\times 18\\) weights and 18 biases, totaling \\(10 \\times 18 + 18 = 198\\) parameters.\n",
    "The second linear layer (`nn.Linear(18, 20)`) has \\(18 \\times 20\\) weights and 20 biases, totaling \\(18 \\times 20 + 20 = 380\\) parameters.\n",
    "The third linear layer (`nn.Linear(20, 5)`) has \\(20 \\times 5\\) weights and 5 biases, totaling \\(20 \\times 5 + 5 = 105\\) parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Update model parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Clear gradients\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do it manually \n",
    "weight0 = model[0].weight\n",
    "\n",
    "weight1 = model[1].weight\n",
    "\n",
    "weight2 = model[2].weight\n",
    "\n",
    "Access the gradients of the weight of each linear layer\n",
    "\n",
    "grads0 = weight0.grad\n",
    "\n",
    "grads1 = weight1.grad\n",
    "\n",
    "grads2 = weight2.grad\n",
    "\n",
    "Update the weights using the learning rate and the gradients\n",
    "\n",
    "weight0 =weight0- lr*grads0\n",
    "\n",
    "weight1 =weight1- lr*grads1\n",
    "\n",
    "weight2 = weight2-lr*grads2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.9086e-03, -2.9293e-03,  4.6748e-03,  7.5734e-04,  6.1277e-03,\n",
      "        -4.7397e-03,  1.1582e-03,  4.3618e-03,  2.2591e-03, -2.9529e-04,\n",
      "         2.0973e-03, -1.5508e-03, -8.1232e-04,  2.5363e-03,  3.4681e-03,\n",
      "        -1.6269e-03,  2.1729e-03, -7.5294e-04,  4.2224e-03, -3.1883e-03,\n",
      "        -4.4115e-03,  1.5361e-03, -1.7820e-03,  2.5023e-03,  4.6675e-03,\n",
      "        -1.7953e-05,  1.9421e-03, -4.7243e-04, -1.1722e-03,  9.5211e-04,\n",
      "         6.2117e-04, -1.0043e-03, -7.0707e-04,  1.6694e-03,  3.4579e-04,\n",
      "         1.1768e-03, -1.3227e-03, -1.0751e-03,  5.2322e-04,  1.3007e-03,\n",
      "        -2.9618e-03,  2.7850e-04, -5.0822e-03, -8.3966e-03, -9.8636e-03,\n",
      "         2.0700e-03, -7.8524e-03, -3.8764e-04, -4.6836e-03,  4.2378e-03,\n",
      "         4.4721e-03, -1.9108e-03, -3.2290e-03,  8.0898e-03, -4.4935e-03,\n",
      "         1.7252e-03,  1.4669e-03, -8.0240e-03,  2.9363e-03,  2.8707e-03,\n",
      "        -2.0191e-03, -4.2979e-04, -3.9457e-03, -1.5092e-03, -2.1893e-03,\n",
      "         4.6655e-03, -3.1833e-03,  8.5809e-04, -2.9165e-03, -3.3803e-03,\n",
      "        -4.0515e-04,  1.2467e-03,  4.3672e-03,  9.0676e-04, -1.1234e-03,\n",
      "        -3.8747e-03, -3.0646e-04,  2.4276e-03, -2.6083e-04,  1.9673e-03,\n",
      "         1.5920e-03, -1.9993e-03, -1.0713e-03, -1.8188e-03, -5.6580e-03,\n",
      "         6.4281e-03, -2.2082e-03, -3.1462e-03, -2.6439e-03,  2.3259e-03,\n",
      "         5.8665e-03,  1.9873e-03, -4.1185e-03,  5.4819e-04,  2.1895e-03,\n",
      "        -5.5597e-03,  4.6419e-03, -3.4501e-04,  3.7054e-03,  7.0452e-04,\n",
      "        -5.5017e-04, -4.8212e-04, -1.3386e-03, -7.4609e-03, -3.2888e-03,\n",
      "        -4.1553e-03, -3.5074e-04, -9.3158e-04,  6.8181e-03, -1.7337e-03,\n",
      "        -2.3091e-03, -3.1902e-03,  3.3309e-03, -4.1378e-03,  2.1921e-03,\n",
      "         2.6269e-03, -1.2313e-03,  3.5597e-03, -2.6771e-03, -3.1727e-03,\n",
      "        -2.0347e-04,  2.2698e-03,  6.4695e-03, -2.4537e-03, -3.3514e-03,\n",
      "        -2.6838e-03, -3.8578e-03,  1.0403e-03, -3.1355e-03, -1.8403e-03,\n",
      "        -4.2209e-03,  1.1224e-03,  9.5917e-03, -7.4106e-03,  6.6392e-04,\n",
      "         6.6422e-03, -1.1312e-02, -8.4325e-04, -9.4383e-03, -5.3197e-03,\n",
      "         2.3464e-03,  1.5217e-03,  1.5788e-03,  4.0802e-04,  1.3114e-03,\n",
      "         1.4641e-03,  7.1698e-04, -1.5588e-03,  1.6628e-04,  6.7985e-04,\n",
      "        -9.7195e-04, -3.7073e-03, -3.3378e-03,  1.9149e-03,  4.1622e-03,\n",
      "        -5.3254e-03,  6.2686e-03, -1.2864e-03,  1.4821e-03, -1.1879e-03,\n",
      "        -1.8602e-03,  6.6142e-04,  4.8673e-03, -2.6905e-03, -4.3323e-03,\n",
      "         5.3893e-03, -2.4174e-03, -2.2145e-03, -2.8294e-03, -3.2664e-03,\n",
      "        -1.0349e-03, -4.8197e-04, -9.6025e-04, -8.2055e-04,  4.6594e-03,\n",
      "        -3.0956e-04, -1.9758e-03, -1.6341e-03, -4.1403e-05, -4.1294e-04,\n",
      "        -2.3852e-03,  3.3378e-04,  8.1490e-04,  9.7032e-04,  3.6532e-04,\n",
      "         2.4073e-03,  9.2323e-04,  7.0730e-04, -1.9478e-03,  3.7822e-03,\n",
      "         4.1920e-03, -1.1133e-03, -9.1578e-04, -7.9263e-03, -1.7376e-03,\n",
      "        -3.8226e-03, -4.5115e-03, -1.0960e-03, -1.1280e-03,  1.5183e-03,\n",
      "         8.4451e-05,  2.8972e-03, -1.7118e-03,  2.9321e-03,  1.0094e-03,\n",
      "         1.2519e-04, -2.2199e-03, -3.6828e-03,  6.9221e-04,  1.1012e-03,\n",
      "        -1.2737e-03, -3.0066e-03, -1.1437e-04,  2.1890e-03,  8.2805e-04,\n",
      "        -2.3515e-03, -1.2885e-03,  2.8188e-03, -1.3925e-04,  9.2938e-04,\n",
      "         5.5599e-04,  1.9387e-03,  5.6865e-04, -4.4813e-04, -3.4839e-04,\n",
      "         7.8330e-04,  7.0487e-04,  2.4592e-04,  1.9332e-03, -9.5386e-04,\n",
      "         2.8798e-04,  2.4708e-03,  3.6814e-04,  1.9748e-04, -5.7070e-03,\n",
      "         7.2822e-04, -8.2485e-03, -4.3485e-03, -5.9713e-03, -2.8537e-03,\n",
      "        -4.0592e-03, -6.6308e-04, -3.5746e-03, -1.2022e-02, -6.9450e-03,\n",
      "        -3.7262e-03, -1.1118e-02, -4.5184e-03, -4.3787e-03, -3.2590e-03,\n",
      "        -6.9479e-03, -6.5681e-03, -7.3888e-03,  2.4253e-03, -1.7635e-04,\n",
      "        -8.7442e-04, -6.3757e-03, -6.6670e-03,  5.7329e-03,  6.9769e-04,\n",
      "        -3.3060e-03, -1.2355e-03,  2.8020e-03, -5.0374e-03, -1.1596e-02,\n",
      "        -1.2474e-02, -1.0459e-05, -3.0303e-03, -8.4303e-03, -3.4727e-03,\n",
      "        -3.0425e-04,  2.2706e-03,  5.1131e-04, -5.9747e-04, -8.5797e-04,\n",
      "        -2.1042e-05,  1.8877e-03,  7.3326e-04, -4.7122e-05,  2.0656e-03,\n",
      "         1.6769e-03,  2.8598e-04,  1.8433e-04, -5.1027e-04,  5.9062e-04,\n",
      "         3.1652e-04,  7.2701e-04,  7.0600e-04, -1.6383e-03, -3.3532e-03,\n",
      "         6.6669e-04,  3.6004e-04, -2.2701e-04, -5.5605e-03,  5.7776e-04,\n",
      "        -7.1401e-04, -1.2151e-03,  6.6266e-03,  4.3122e-03, -1.3929e-03,\n",
      "        -5.6991e-03, -3.5615e-03, -1.7862e-03, -4.2893e-03, -1.0902e-03,\n",
      "         1.6093e-03,  2.8289e-03,  3.2753e-03,  4.4812e-03,  7.4316e-04,\n",
      "         2.3993e-03,  2.7172e-03,  1.1475e-02,  6.2814e-03,  4.3964e-04,\n",
      "         1.1390e-02,  1.3861e-02,  0.0000e+00,  8.0532e-03, -5.1800e-04,\n",
      "         9.9104e-04,  2.7602e-03,  6.7700e-05,  2.4324e-03, -3.7575e-03,\n",
      "         6.1218e-06, -1.5640e-03, -2.3379e-03, -1.5826e-03, -1.5758e-03,\n",
      "        -7.7377e-03, -1.3198e-03,  1.4733e-03, -6.8164e-03, -4.4103e-03,\n",
      "        -7.2727e-04, -3.2140e-03,  4.0515e-04, -3.6934e-04, -2.8017e-03,\n",
      "         6.3768e-05, -1.7072e-03, -2.6389e-04, -1.8019e-03, -5.5925e-04,\n",
      "         3.2334e-04, -7.5449e-04, -3.1568e-04,  7.6532e-04, -3.9530e-04,\n",
      "        -1.3600e-03, -3.4707e-04, -5.5298e-04, -1.1423e-03, -6.3869e-04,\n",
      "        -6.3960e-05,  7.5888e-05, -1.1248e-04, -1.6056e-03,  0.0000e+00,\n",
      "        -4.5415e-03, -3.2659e-04, -1.7660e-03, -3.2520e-04, -4.5831e-03,\n",
      "        -2.0883e-03,  6.4763e-04,  1.0895e-03, -1.7715e-03, -1.7300e-03,\n",
      "        -7.7379e-04, -3.7338e-03, -5.4940e-03, -6.9267e-03, -1.4113e-04,\n",
      "        -9.5501e-04, -5.0781e-03, -1.6168e-03,  6.1371e-05,  3.0198e-03,\n",
      "        -2.4962e-03,  8.6059e-04, -5.4117e-03, -3.2478e-04,  3.6507e-03,\n",
      "         2.1073e-03, -3.0625e-04, -2.5445e-03, -9.9352e-04, -1.9290e-03,\n",
      "        -5.4728e-03, -6.4756e-03, -7.6073e-04,  2.1133e-04, -2.8575e-03,\n",
      "        -2.2339e-03, -1.1379e-04,  5.9741e-05,  1.5782e-04, -6.7990e-05,\n",
      "         5.8526e-04,  6.7433e-04, -3.0189e-04, -5.4468e-04,  5.4945e-04,\n",
      "        -3.5461e-04, -3.9862e-04,  1.2142e-03,  1.0243e-03,  8.8653e-04,\n",
      "        -5.9029e-04, -3.9128e-04,  8.2856e-04,  1.1034e-04, -1.7068e-03,\n",
      "         2.1835e-03,  1.0231e-03,  8.9557e-04, -1.0878e-03,  2.8768e-03,\n",
      "         1.2484e-03,  2.2167e-05, -2.9954e-03,  2.6860e-04,  2.0791e-03,\n",
      "        -1.5237e-03,  4.9091e-05, -5.0677e-03,  4.8915e-04,  3.7522e-03,\n",
      "        -3.5805e-03, -6.0898e-04,  1.0099e-04,  2.3967e-04, -4.9149e-04,\n",
      "        -2.0524e-03,  2.7481e-04, -9.7956e-04,  6.5767e-04, -2.3514e-04,\n",
      "        -9.3623e-05,  0.0000e+00, -4.6395e-04, -1.2037e-03, -4.7022e-04,\n",
      "        -7.7829e-04, -1.1773e-05,  0.0000e+00, -3.0267e-03,  0.0000e+00,\n",
      "        -1.4376e-03,  6.0509e-04, -7.5860e-04,  2.0922e-03, -3.5168e-04,\n",
      "         3.0649e-03,  1.8957e-04,  5.8797e-05, -8.0110e-04, -2.0980e-04,\n",
      "         3.8703e-04,  1.5879e-03,  9.3348e-04,  7.8274e-04,  0.0000e+00,\n",
      "         1.4186e-03,  2.8670e-03, -7.8482e-04,  1.5974e-03, -6.8088e-04,\n",
      "         3.8432e-04,  7.2921e-04,  1.7711e-03,  1.3805e-03,  6.1299e-04,\n",
      "         1.1584e-04,  1.5230e-03,  5.0653e-04,  1.0520e-03,  1.9211e-03,\n",
      "         4.3965e-03,  4.5432e-03,  3.7939e-04,  1.7454e-03,  3.5619e-03,\n",
      "         2.6267e-04,  6.1848e-04,  1.6722e-05,  0.0000e+00,  0.0000e+00,\n",
      "         3.2342e-03,  2.8742e-04,  3.2430e-04,  0.0000e+00,  1.2045e-04,\n",
      "         1.2272e-03,  3.9249e-04,  4.3003e-04,  4.7208e-03,  2.2869e-03,\n",
      "         1.9334e-05,  3.2511e-04,  6.6712e-04,  6.7917e-04, -1.0130e-03,\n",
      "        -1.3840e-04,  1.1480e-03,  2.2129e-04, -7.0819e-05,  2.4776e-04,\n",
      "         7.5072e-04, -1.6814e-04,  3.5067e-04,  1.5522e-04,  3.4208e-03,\n",
      "        -7.5300e-06,  6.5404e-04, -1.2871e-03, -5.1421e-04, -1.3113e-03,\n",
      "         1.4625e-04,  3.0337e-05, -1.9805e-03, -7.2212e-04,  3.5809e-03,\n",
      "         3.9385e-03,  1.6058e-05, -1.6798e-03,  5.1654e-03, -2.3278e-03,\n",
      "        -2.1742e-03,  4.2810e-03,  5.5943e-03,  1.3550e-04, -2.7881e-03,\n",
      "        -2.8525e-03,  2.9016e-03,  1.4008e-03,  5.2988e-04, -1.6218e-04,\n",
      "        -5.7087e-04, -2.9376e-03, -2.6079e-03, -7.1379e-04,  1.3595e-03,\n",
      "        -3.0752e-04, -1.3490e-03, -6.7069e-04,  2.3736e-03,  1.6982e-04,\n",
      "        -4.4452e-03, -2.0747e-04,  5.7337e-03,  5.9601e-03, -4.4989e-04,\n",
      "        -6.8245e-04,  2.1333e-03,  1.5372e-03,  1.5720e-03,  2.5574e-03,\n",
      "        -2.0642e-02, -1.5588e-02,  5.2553e-04, -3.7257e-03,  1.4375e-02,\n",
      "        -1.1712e-02, -4.4921e-04, -1.0068e-02, -4.7891e-03,  1.5852e-03,\n",
      "         2.6188e-03, -1.4183e-03,  2.6252e-03,  7.2169e-03,  3.5424e-03,\n",
      "         2.9796e-03,  5.0411e-03,  2.8651e-03,  1.2069e-02,  6.9204e-03,\n",
      "        -2.6742e-03,  1.2368e-02,  5.8454e-03,  5.2327e-03,  6.9440e-03,\n",
      "         1.1093e-02,  6.0083e-04, -2.4329e-04,  4.9957e-03,  1.1132e-03,\n",
      "         4.0531e-03,  6.2131e-04,  1.9162e-03,  2.4544e-03,  9.8614e-04,\n",
      "         2.4069e-04,  3.4906e-03,  5.2328e-03,  9.3509e-03,  4.8511e-03,\n",
      "         7.7422e-03,  4.9323e-03,  2.3971e-03,  3.1319e-03, -3.7342e-03,\n",
      "        -9.3599e-03, -4.2267e-04,  3.3109e-03,  7.8093e-03,  1.3562e-03,\n",
      "        -1.9272e-03,  5.9710e-04,  1.8400e-03,  9.3630e-04,  6.7199e-04,\n",
      "        -8.1380e-05,  7.7907e-03,  9.7022e-04, -2.1510e-02, -1.0972e-04,\n",
      "        -1.5001e-02, -3.1894e-02, -7.6159e-03, -3.4996e-03,  2.6034e-03,\n",
      "        -1.4069e-04,  1.0720e-03, -1.0430e-02, -1.0087e-02, -4.2443e-03,\n",
      "        -6.4870e-03, -2.7965e-03, -5.6215e-03, -3.5350e-03, -2.7653e-03,\n",
      "        -3.2015e-03, -3.4971e-03, -4.7824e-03,  2.4438e-03, -1.0205e-02,\n",
      "         7.7845e-03,  1.4078e-02, -2.0798e-03, -2.1569e-03, -1.1214e-02,\n",
      "        -6.6701e-03, -2.1020e-03,  4.3920e-03, -3.1119e-03,  2.9304e-04,\n",
      "         5.0496e-04,  8.4436e-04,  9.3498e-04,  2.2230e-03,  8.2641e-04,\n",
      "         1.5961e-03, -6.9544e-03, -2.1113e-03, -2.3540e-03, -1.4569e-03,\n",
      "         2.1483e-03,  5.1557e-04,  1.4533e-03, -2.7081e-03,  5.4008e-03,\n",
      "         5.0779e-03,  8.5185e-04,  2.9703e-03,  3.9375e-04,  1.4819e-03,\n",
      "         3.8561e-03,  7.3375e-04,  9.3036e-04, -2.0787e-03,  2.8075e-04,\n",
      "         1.4461e-03, -8.2987e-04,  6.9075e-04,  6.2050e-02,  3.5559e-02,\n",
      "        -6.2619e-02, -2.5819e-02, -9.1699e-03])\n"
     ]
    }
   ],
   "source": [
    "updated_outputs = model(input_data)\n",
    "loss = criterion(updated_outputs, targets)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Extract gradients\n",
    "gradients = []\n",
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        gradients.append(param.grad.view(-1))\n",
    "gradients = torch.cat(gradients)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2139e-01, -2.1565e-01, -1.5174e-01, -6.9622e-02,  1.3354e-01],\n",
       "        [ 1.4627e-01, -1.3089e-01, -1.5582e-01,  9.7402e-03,  1.3720e-01],\n",
       "        [ 9.0151e-02, -1.4322e-01, -2.1309e-01,  2.9169e-02,  2.6267e-01],\n",
       "        [ 1.6294e-01, -2.5998e-01, -4.4575e-02, -8.5716e-02,  1.0176e-01],\n",
       "        [ 1.9883e-01, -1.9001e-01, -1.4166e-01, -4.4087e-02,  1.5521e-01],\n",
       "        [ 1.6569e-01, -1.5531e-01, -1.6349e-01, -8.5988e-03,  1.2282e-01],\n",
       "        [ 1.2570e-01, -2.1066e-01, -5.2862e-02, -8.1039e-02,  2.4847e-03],\n",
       "        [ 1.6965e-01, -2.0882e-01, -1.9724e-01,  1.9199e-02,  2.4841e-01],\n",
       "        [ 1.1911e-01, -2.2902e-01, -1.1128e-01, -1.1262e-01,  6.0103e-02],\n",
       "        [ 1.1484e-01, -2.1097e-01, -1.3596e-01, -5.1027e-02,  1.8778e-01],\n",
       "        [ 2.3638e-01, -1.4357e-01, -1.4383e-01,  4.4006e-02,  1.8750e-01],\n",
       "        [ 1.0246e-01, -1.7679e-01, -1.6548e-01, -4.6357e-02,  1.6594e-01],\n",
       "        [ 1.0391e-01, -1.7545e-01, -1.8134e-01, -4.3168e-02,  1.7868e-01],\n",
       "        [ 1.5279e-01, -1.9179e-01, -1.5385e-01,  1.1216e-02,  1.8203e-01],\n",
       "        [-1.6864e-02, -1.9087e-01, -2.3478e-03, -1.7831e-02, -5.9303e-02],\n",
       "        [ 1.2108e-01, -1.4780e-01, -1.7245e-01,  3.8773e-02,  1.9741e-01],\n",
       "        [ 1.5708e-01, -1.4383e-01, -1.9003e-01,  2.0808e-02,  1.9029e-01],\n",
       "        [ 1.0101e-01, -1.8910e-01, -8.4894e-02, -5.1853e-02,  1.2129e-01],\n",
       "        [ 6.8808e-02, -3.3402e-01,  1.5146e-01, -1.1256e-01, -7.9432e-02],\n",
       "        [ 1.9388e-01, -2.6424e-01, -1.2384e-01, -1.0977e-01,  2.1842e-02],\n",
       "        [ 9.1952e-02, -1.9156e-01, -1.1339e-01, -5.5198e-02,  7.2073e-02],\n",
       "        [ 1.4916e-01, -1.3941e-01, -2.0330e-01,  2.6237e-02,  2.4416e-01],\n",
       "        [ 7.2605e-02, -1.9330e-01,  4.6847e-02,  4.9411e-02, -3.7831e-02],\n",
       "        [ 9.7754e-02, -2.0642e-01, -5.8390e-02, -4.7299e-02,  6.0014e-02],\n",
       "        [ 2.2984e-01, -2.1295e-01, -1.1184e-01,  6.7359e-05,  1.2184e-01],\n",
       "        [ 7.4642e-02, -1.9111e-01, -1.1098e-01, -4.8307e-02,  6.8921e-02],\n",
       "        [ 6.6484e-02, -2.1827e-01, -1.1953e-01, -8.7426e-02,  1.1360e-01],\n",
       "        [ 1.3623e-01, -2.4157e-01, -1.2153e-02, -6.0110e-02,  5.3664e-02],\n",
       "        [ 2.9639e-01, -2.0500e-01, -9.3963e-02, -7.4064e-02,  1.0526e-01],\n",
       "        [ 1.8908e-01, -1.8159e-01, -1.4904e-01,  2.5339e-05,  1.8916e-01],\n",
       "        [ 1.6443e-01, -1.6636e-01, -1.6323e-01, -3.5056e-02,  1.5401e-01],\n",
       "        [ 1.2826e-01, -2.2501e-01, -2.0662e-01, -6.2206e-03,  2.1555e-01],\n",
       "        [ 1.9013e-01, -2.7949e-01, -1.5554e-01, -9.6227e-02,  1.0549e-01],\n",
       "        [ 2.0225e-01, -2.5365e-01, -1.5121e-01, -5.9477e-02,  1.8690e-01],\n",
       "        [ 1.4800e-01, -1.7928e-01, -1.8619e-01, -7.0707e-03,  2.2102e-01],\n",
       "        [ 1.8721e-01, -1.8948e-01, -1.2282e-01, -7.1466e-02,  1.2252e-01],\n",
       "        [ 1.7384e-01, -1.5872e-01, -1.5265e-01, -1.8140e-02,  1.5743e-01],\n",
       "        [ 1.5633e-01, -1.7322e-01, -1.4758e-01, -6.5464e-03,  1.5075e-01],\n",
       "        [ 1.7462e-01, -2.1472e-01, -1.2364e-01, -1.0271e-01,  8.8161e-02],\n",
       "        [ 2.1644e-01, -1.6507e-01, -1.8583e-01, -4.9437e-02,  1.6021e-01],\n",
       "        [ 1.7986e-01, -1.8350e-01, -1.5138e-01, -5.1651e-02,  1.4209e-01],\n",
       "        [ 1.8958e-01, -1.9489e-01, -1.0578e-01, -4.4451e-02,  1.6603e-01],\n",
       "        [ 2.5385e-01, -1.7536e-01, -1.8894e-01, -4.3447e-02,  1.5442e-01],\n",
       "        [ 1.7032e-01, -1.6093e-01, -1.9794e-01,  2.4368e-03,  1.9736e-01],\n",
       "        [ 1.2211e-01, -1.8895e-01, -1.3260e-01, -3.7518e-02,  1.6663e-01],\n",
       "        [ 2.3493e-01, -1.4270e-01, -1.9954e-01, -1.3535e-02,  1.7447e-01],\n",
       "        [ 1.1270e-01, -2.1279e-01, -1.2075e-01, -7.4495e-02,  1.2233e-01],\n",
       "        [ 7.6251e-02, -1.9548e-01, -9.5311e-02, -5.6672e-02,  8.2125e-02],\n",
       "        [ 1.9067e-01, -1.3999e-01, -1.2795e-01, -1.1117e-02,  1.2626e-01],\n",
       "        [ 1.5830e-01, -1.8566e-01, -1.5909e-01, -3.2086e-02,  2.3154e-01],\n",
       "        [ 1.5214e-01, -1.2649e-01, -1.5538e-01,  2.8559e-02,  1.1087e-01],\n",
       "        [ 1.7378e-01, -1.8889e-01, -1.6549e-01, -7.3644e-02,  1.4524e-01],\n",
       "        [ 1.8025e-01, -1.9612e-01, -1.4478e-01, -8.5251e-02,  1.2534e-01],\n",
       "        [-8.7175e-02, -2.2922e-01, -2.6014e-03, -4.2376e-02, -3.5110e-02],\n",
       "        [ 8.4101e-02, -2.2329e-01,  7.4406e-02, -5.1764e-02, -8.4643e-02],\n",
       "        [ 1.8629e-01, -1.7157e-01, -1.5918e-01,  4.2400e-02,  2.1245e-01],\n",
       "        [ 1.8106e-01, -1.6036e-01, -1.8790e-01, -3.0290e-02,  1.7621e-01],\n",
       "        [ 4.8446e-02, -1.6234e-01, -1.2680e-01,  2.4570e-02,  9.4313e-02],\n",
       "        [ 1.1603e-01, -1.5198e-01, -2.0152e-01,  9.0905e-03,  2.4515e-01],\n",
       "        [ 1.5166e-01, -1.6389e-01, -1.6974e-01,  5.3774e-03,  2.1807e-01],\n",
       "        [ 9.8351e-02, -2.0425e-01, -1.2691e-01, -4.5754e-02,  1.5910e-01],\n",
       "        [ 9.0237e-02, -2.0028e-01, -1.2315e-01, -5.3733e-02,  1.2913e-01],\n",
       "        [ 1.5302e-01, -1.7566e-01, -1.1627e-01, -1.3313e-02,  1.5521e-01],\n",
       "        [ 4.0078e-02, -2.1792e-01, -1.9189e-01, -9.8537e-02,  1.9262e-01],\n",
       "        [ 1.6508e-01, -1.8095e-01, -1.5491e-01, -4.1113e-02,  1.4376e-01],\n",
       "        [ 1.5404e-01, -1.8633e-01, -1.4371e-01, -5.6200e-02,  1.3882e-01],\n",
       "        [ 1.5362e-01, -2.3963e-01, -1.5664e-01, -7.3990e-02,  1.9094e-01],\n",
       "        [ 1.5531e-01, -2.4298e-01, -8.9223e-02, -8.8234e-02,  1.4841e-01],\n",
       "        [ 1.5905e-01, -1.8787e-01, -1.2390e-01, -3.8535e-02,  1.8002e-01],\n",
       "        [ 2.2407e-01, -3.4179e-01, -5.8601e-02, -2.0127e-01,  1.3405e-01],\n",
       "        [ 1.3258e-01, -2.4572e-01, -1.4724e-01, -6.4124e-02,  1.5924e-01],\n",
       "        [ 1.8294e-01, -2.3331e-01, -1.7106e-01, -7.1852e-03,  1.9921e-01],\n",
       "        [ 1.9454e-01, -1.6753e-01, -1.5805e-01, -3.6360e-02,  1.5309e-01],\n",
       "        [ 5.5460e-02, -2.1199e-01, -1.2430e-01, -9.8821e-02,  1.1989e-01],\n",
       "        [ 1.4293e-01, -1.9278e-01, -1.6344e-01, -2.6484e-02,  2.0021e-01],\n",
       "        [ 1.3574e-02, -2.6974e-01, -3.4495e-02, -4.2219e-02,  8.7287e-03],\n",
       "        [ 1.0145e-01, -1.2477e-01, -2.3464e-01,  4.5179e-02,  2.7026e-01],\n",
       "        [ 1.5529e-01, -1.3437e-01, -1.8846e-01,  4.3354e-02,  1.9561e-01],\n",
       "        [ 6.1761e-02, -3.1789e-01, -1.4253e-01, -8.1440e-02,  1.4053e-01],\n",
       "        [ 8.2694e-02, -1.8307e-01, -1.4402e-01, -1.6509e-02,  1.7309e-01],\n",
       "        [ 2.0449e-01, -2.7770e-01, -4.9986e-02, -1.0216e-01,  1.0459e-01],\n",
       "        [ 2.2046e-02, -1.2527e-01,  1.8274e-02,  7.6715e-02, -5.3012e-02],\n",
       "        [ 1.7409e-01, -1.5832e-01, -1.8482e-01, -2.4819e-02,  1.6803e-01],\n",
       "        [ 1.8750e-01, -1.3731e-01, -1.6415e-01,  5.9487e-03,  1.1512e-01],\n",
       "        [ 1.1451e-01, -1.9418e-01, -3.3831e-02, -5.0993e-02,  7.1409e-02],\n",
       "        [ 2.1184e-01, -1.5287e-01, -1.9778e-01, -1.4600e-02,  1.9262e-01],\n",
       "        [ 1.2397e-01, -2.8287e-01, -9.7356e-02, -8.5989e-02,  5.4681e-02],\n",
       "        [ 1.4476e-01, -1.8397e-01, -7.7842e-02, -5.3138e-02,  6.8032e-02],\n",
       "        [ 1.3265e-01, -1.3632e-01, -1.5051e-01,  9.8369e-03,  1.3039e-01],\n",
       "        [ 1.9742e-01, -2.4348e-01, -9.1015e-02, -7.1817e-02,  1.6523e-01],\n",
       "        [ 1.2506e-01, -2.1187e-01, -9.2214e-02, -9.3943e-02,  7.8271e-02],\n",
       "        [ 1.3278e-01, -1.6904e-01, -1.6971e-01, -3.4568e-02,  1.5840e-01],\n",
       "        [ 1.8546e-01, -2.0165e-01, -1.9820e-01, -3.2852e-02,  2.0216e-01],\n",
       "        [ 1.5886e-01, -1.3510e-01, -1.8635e-01,  2.3478e-02,  2.0661e-01],\n",
       "        [ 1.8135e-01, -2.1172e-01, -1.0852e-01, -3.7847e-02,  1.5899e-01],\n",
       "        [ 1.7520e-01, -1.8251e-01, -1.6116e-01, -3.2314e-02,  1.3854e-01],\n",
       "        [ 1.5648e-01, -2.5459e-01, -6.3647e-02, -7.1821e-02,  8.1136e-02],\n",
       "        [ 1.9583e-01, -1.4050e-01, -1.0856e-01,  9.4443e-03,  1.2501e-01],\n",
       "        [ 1.1523e-01, -1.9239e-01, -1.3362e-01, -5.6908e-02,  1.5068e-01],\n",
       "        [ 7.1245e-02, -2.2774e-01, -1.8665e-01, -7.5075e-03,  2.4187e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 4, 2, 3, 4, 2, 2, 1, 2, 2, 0, 1, 2, 2, 4, 3, 0, 2, 2, 1, 3, 2, 0,\n",
       "        1, 0, 0, 2, 3, 4, 2, 1, 1, 3, 2, 4, 4, 3, 3, 4, 3, 3, 3, 3, 2, 3, 1, 3,\n",
       "        0, 1, 0, 4, 0, 2, 0, 3, 0, 2, 3, 2, 0, 4, 2, 4, 4, 0, 4, 4, 3, 3, 3, 4,\n",
       "        4, 2, 1, 2, 2, 4, 1, 1, 4, 4, 0, 2, 2, 2, 1, 0, 3, 0, 4, 4, 3, 1, 4, 3,\n",
       "        0, 4, 4, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'updated_model.pth')\n",
    "model.load_state_dict(torch.load('updated_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the training loop\n",
    "\n",
    "let's do it for regression problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9845340074971318\n",
      "Epoch 2, Loss: 0.962383781094104\n",
      "Epoch 3, Loss: 0.976639011874795\n",
      "Epoch 4, Loss: 0.99384387396276\n",
      "Epoch 5, Loss: 0.9766926039010286\n",
      "Epoch 6, Loss: 0.998492281883955\n",
      "Epoch 7, Loss: 0.9557647313922644\n",
      "Epoch 8, Loss: 0.9578550774604082\n",
      "Epoch 9, Loss: 0.9598546903580427\n",
      "Epoch 10, Loss: 0.949790159240365\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "features = torch.randn(1000, 10)\n",
    "labels = torch.randn(1000)\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_features, batch_labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs.squeeze(), batch_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "between layers activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x:max(x,0) # it overcomes the vaniching garidient problem\n",
    "# that apprears when using sigmoid \n",
    "relunn = nn.ReLU()\n",
    "\n",
    "# leaky relu for pos the same as relu for neg it multiplies them\n",
    "# by a small coeff defaulted to 0.01 like this grad has non null value \n",
    "# for neg values\n",
    "leaky =nn.LeakyReLU(negative_slope=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optim.SGD(model.parameters(),lr=0.01,momentum=0.95)\n",
    "# the momentum controls the inertia of the optimizer which \n",
    "# helps escape the local min\n",
    "# the momentom keeps the step size large when previous is large\n",
    "# even if gradient is small typically between 0.85 to 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3246e-06, grad_fn=<MinBackward1>) tensor(0.9999, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "# we can initialize the layer weight with any dist\n",
    "layer = nn.Linear(64,128)\n",
    "nn.init.uniform_(layer.weight)\n",
    "\n",
    "print(layer.weight.min(),layer.weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3246e-06, grad_fn=<MinBackward1>) tensor(0.9999, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight.min(),layer.weight.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning : usinf a model traing on a first task for a second similar task to accelerate the training process\n",
    "\n",
    "- example we train a model on big dataset for salries in us and we retrain it on a small dataset for europ so the coefficient have initial values of us which will make it converge quickly\n",
    "\n",
    "fine tuning is a type of transfer learing: we laod weights from previous model then we use smaller learning rate\n",
    " - not evry layer is trained ( we freeze some)\n",
    " - rule of themb: freeze early layers of netw and fine-tune layers close to output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if name ==\"0.weight\":\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have calculated the training loss before \n",
    "no we calculate the validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
