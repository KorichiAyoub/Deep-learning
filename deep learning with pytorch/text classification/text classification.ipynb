{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'ayoub',\n",
       " 'from',\n",
       " 'ensia',\n",
       " 'the',\n",
       " 'national',\n",
       " 'higher',\n",
       " 'shchool',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'I',\n",
       " 'created',\n",
       " 'this',\n",
       " 'for',\n",
       " 'learning',\n",
       " 'purposes',\n",
       " 'and',\n",
       " 'help',\n",
       " 'students',\n",
       " 'when',\n",
       " 'doing',\n",
       " 'ai',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def tokenize(text):\n",
    "    # Split text into words using whitespace and punctuation as delimiters\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "tokens = tokenize(\"i am ayoub, from ensia the national higher shchool of ai, I created this for learning purposes and help students when doing ai tasks\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ayoub',\n",
       " 'ensia',\n",
       " 'national',\n",
       " 'higher',\n",
       " 'shchool',\n",
       " 'ai',\n",
       " 'created',\n",
       " 'learning',\n",
       " 'purposes',\n",
       " 'help',\n",
       " 'students',\n",
       " 'ai',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'ayoub',\n",
       " 'from',\n",
       " 'ensia',\n",
       " 'the',\n",
       " 'nation',\n",
       " 'higher',\n",
       " 'shchool',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'i',\n",
       " 'creat',\n",
       " 'thi',\n",
       " 'for',\n",
       " 'learn',\n",
       " 'purpos',\n",
       " 'and',\n",
       " 'help',\n",
       " 'student',\n",
       " 'when',\n",
       " 'do',\n",
       " 'ai',\n",
       " 'task']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() # doesnt work for slangs like darija ...\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'ai', 'i', 'ai']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rare words not always needed tho \n",
    "from nltk.probability import FreqDist\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 2\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token]>= threshold]\n",
    "common_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.33721386,\n",
       "        0.        , 0.        , 0.33721386, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33721386, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33721386, 0.        ,\n",
       "        0.33721386, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33721386, 0.33721386,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.4516721 ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.25451241, 0.31546157, 0.        , 0.31546157, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.50902482, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31546157, 0.31546157, 0.31546157, 0.42253658,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.25573548, 0.        , 0.31697754, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.31697754,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.31697754,\n",
       "        0.25573548, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31697754, 0.        ,\n",
       "        0.31697754, 0.25573548, 0.        , 0.        , 0.        ,\n",
       "        0.31697754, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31697754, 0.31697754],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35355339, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35355339, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35355339, 0.        , 0.        , 0.35355339,\n",
       "        0.        , 0.35355339, 0.35355339, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35355339, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.35355339, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.36757597, 0.        , 0.        , 0.36757597, 0.        ,\n",
       "        0.36757597, 0.        , 0.36757597, 0.        , 0.        ,\n",
       "        0.29655798, 0.        , 0.29655798, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.36757597,\n",
       "        0.        , 0.29655798, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24616991,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "CountVect = CountVectorizer()\n",
    "TfidfVect = TfidfVectorizer()\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The sky is blue and the sun is shining brightly.\",\n",
    "    \"I enjoy reading books and listening to music in my free time.\",\n",
    "    \"Learning new things every day keeps life interesting.\",\n",
    "    \"Coffee is my favorite drink, especially in the morning.\"\n",
    "]\n",
    "\n",
    "bagOfwords = CountVect.fit_transform(corpus)\n",
    "vectorized_x = TfidfVect.fit_transform(corpus)\n",
    "print(CountVect.get_feature_names_out()[:5])\n",
    "print(bagOfwords.toarray()[0, :5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def encode_sentences(sentences):\n",
    "    print(\"Preprocessed sentences:\", sentences)\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer\n",
    "\n",
    "def extract_sentences(data):\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        # freq_dist = FreqDist(tokens)\n",
    "        # threshold = 2\n",
    "        # tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define a custom PyTorch dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def text_processing_pipeline(text):\n",
    "    # Preprocess the text\n",
    "    tokens = preprocess_sentences(text)\n",
    "    \n",
    "    # Encode the preprocessed sentences\n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens)\n",
    "    \n",
    "    # Create a PyTorch dataset\n",
    "    dataset = TextDataset(encoded_sentences)\n",
    "    \n",
    "    # Create a dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    return dataloader, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentences: ['first text data .', 'anoth one .']\n",
      "tensor([0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "text_data = \"This is the first text data. And here is another one.\"\n",
    "sentences = extract_sentences(text_data)\n",
    "dataloader, vectorizer = text_processing_pipeline(sentences)\n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2718, -1.0707,  0.4272,  0.7276, -0.1287,  0.6916,  1.3477,  1.9173,\n",
      "         -0.6723, -0.1657],\n",
      "        [ 0.0402,  1.5819, -0.9921, -1.3865,  0.4599, -1.2141,  0.0219, -1.0111,\n",
      "          0.1189, -0.6732],\n",
      "        [ 1.5348,  0.2376,  0.8647,  0.4961,  1.2731,  0.0022, -1.0568,  1.2796,\n",
      "         -0.8611,  1.5722],\n",
      "        [-0.5900, -0.0346,  0.0956, -0.6735, -1.2555, -0.5936,  1.4562,  0.5631,\n",
      "          0.6267,  0.4761],\n",
      "        [-0.2718, -1.0707,  0.4272,  0.7276, -0.1287,  0.6916,  1.3477,  1.9173,\n",
      "         -0.6723, -0.1657],\n",
      "        [-0.6252, -0.7574,  1.2452,  0.5137,  1.0739, -0.1121, -1.1330,  0.3104,\n",
      "          1.2396,  0.6651]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "words = ['the','cat','sat','on','the','mat']\n",
    "word_to_idx = {word: idx for idx, word in enumerate((words))}\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings =len(words), embedding_dim=10)\n",
    "output = embedding(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in cnns a **filter** is a small matrix we slide over the input \n",
    "and **stride** is the number of positions the filter moves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SentimentAnalysisCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc =  nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2)\n",
    "        return self.fc(conved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "vocab = [\"i\",\"slow\", \"love\",\"shallow\",\"predictable\", \"this\", \"book\", \"do\", \"not\",\"adored\",\"absolutely\",\"breathtaking\", \"like\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(word_to_idx)\n",
    "embed_dim = 10\n",
    "\n",
    "book_samples = [\n",
    "    (\"The story was captivating and kept me hooked until the end.love\".split(), 1),\n",
    "    (\"I found the characters shallow and the plot predictable.\".split(), 0)\n",
    "]\n",
    "\n",
    "model = SentimentAnalysisCNN(vocab_size, embed_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'story', 'was', 'captivating', 'and', 'kept', 'me', 'hooked', 'until', 'the', 'end.love'] 1\n",
      "['I', 'found', 'the', 'characters', 'shallow', 'and', 'the', 'plot', 'predictable.'] 0\n",
      "['This', 'movie', 'is', 'a', 'masterpiece', 'of', 'storytelling.'] 1\n",
      "['The', 'acting', 'was', 'terrible', 'and', 'the', 'dialogue', 'felt', 'forced.'] 0\n",
      "['I', \"couldn't\", 'stop', 'laughing', 'throughout', 'the', 'entire', 'film.love'] 1\n",
      "['The', 'special', 'effects', 'were', 'impressive,', 'but', 'the', 'story', 'lacked', 'depth.'] 0\n",
      "['The', 'soundtrack', 'perfectly', 'complemented', 'the', 'mood', 'of', 'the', 'film.'] 1\n",
      "['The', 'pacing', 'was', 'too', 'slow', 'and', 'made', 'the', 'movie', 'drag', 'on.'] 0\n",
      "['I', 'absolutely', 'adored', 'this', 'book.', \"It's\", 'a', 'must-read!'] 1\n",
      "['The', 'ending', 'of', 'the', 'movie', 'left', 'me', 'in', 'tears.'] 1\n",
      "['The', 'plot', 'twists', 'kept', 'me', 'on', 'the', 'edge', 'of', 'my', 'seat.'] 1\n",
      "['The', 'writing', 'style', 'was', 'too', 'verbose', 'for', 'my', 'liking.'] 0\n",
      "['The', 'cinematography', 'in', 'this', 'film', 'is', 'breathtaking.'] 1\n",
      "['The', 'dialogue', 'felt', 'stilted', 'and', 'unnatural.'] 0\n",
      "['The', 'protagonist', 'was', 'relatable', 'and', 'well-developed.'] 1\n",
      "['The', 'pacing', 'of', 'the', 'story', 'felt', 'rushed', 'and', 'disjointed.'] 0\n",
      "['I', \"couldn't\", 'put', 'this', 'book', 'down', 'until', 'I', 'finished', 'it.'] 1\n",
      "['The', 'special', 'effects', 'were', 'unconvincing', 'and', 'cheesy.'] 0\n",
      "['The', 'character', 'development', 'was', 'shallow', 'and', 'one-dimensional.'] 0\n",
      "['The', 'soundtrack', 'added', 'depth', 'and', 'emotion', 'to', 'the', 'scenes.'] 1\n",
      "['I', 'was', 'disappointed', 'by', 'the', 'lackluster', 'ending.'] 0\n",
      "['The', 'acting', 'performances', 'were', 'stellar', 'across', 'the', 'board.'] 1\n",
      "['The', 'story', 'had', 'me', 'guessing', 'until', 'the', 'very', 'end.'] 1\n",
      "['The', 'dialogue', 'was', 'witty', 'and', 'engaging.'] 1\n",
      "['The', 'pacing', 'of', 'the', 'film', 'was', 'too', 'slow', 'for', 'my', 'taste.'] 0\n",
      "['The', 'setting', 'was', 'vividly', 'described', 'and', 'immersive.'] 1\n",
      "['The', 'plot', 'was', 'predictable', 'and', 'clichéd.'] 0\n",
      "['The', 'emotional', 'depth', 'of', 'the', 'characters', 'resonated', 'with', 'me.'] 1\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences with labels\n",
    "data = [\n",
    "    (\"The story was captivating and kept me hooked until the end.love\".split(), 1),\n",
    "    (\"I found the characters shallow and the plot predictable.\".split(), 0),\n",
    "    (\"This movie is a masterpiece of storytelling.\".split(), 1),\n",
    "    (\"The acting was terrible and the dialogue felt forced.\".split(), 0),\n",
    "    (\"I couldn't stop laughing throughout the entire film.love\".split(), 1),\n",
    "    (\"The special effects were impressive, but the story lacked depth.\".split(), 0),\n",
    "    (\"The soundtrack perfectly complemented the mood of the film.\".split(), 1),\n",
    "    (\"The pacing was too slow and made the movie drag on.\".split(), 0),\n",
    "    (\"I absolutely adored this book. It's a must-read!\".split(), 1),\n",
    "    (\"The ending of the movie left me in tears.\".split(), 1),\n",
    "    (\"The plot twists kept me on the edge of my seat.\".split(), 1),\n",
    "    (\"The writing style was too verbose for my liking.\".split(), 0),\n",
    "    (\"The cinematography in this film is breathtaking.\".split(), 1),\n",
    "    (\"The dialogue felt stilted and unnatural.\".split(), 0),\n",
    "    (\"The protagonist was relatable and well-developed.\".split(), 1),\n",
    "    (\"The pacing of the story felt rushed and disjointed.\".split(), 0),\n",
    "    (\"I couldn't put this book down until I finished it.\".split(), 1),\n",
    "    (\"The special effects were unconvincing and cheesy.\".split(), 0),\n",
    "    (\"The character development was shallow and one-dimensional.\".split(), 0),\n",
    "    (\"The soundtrack added depth and emotion to the scenes.\".split(), 1),\n",
    "    (\"I was disappointed by the lackluster ending.\".split(), 0),\n",
    "    (\"The acting performances were stellar across the board.\".split(), 1),\n",
    "    (\"The story had me guessing until the very end.\".split(), 1),\n",
    "    (\"The dialogue was witty and engaging.\".split(), 1),\n",
    "    (\"The pacing of the film was too slow for my taste.\".split(), 0),\n",
    "    (\"The setting was vividly described and immersive.\".split(), 1),\n",
    "    (\"The plot was predictable and clichéd.\".split(), 0),\n",
    "    (\"The emotional depth of the characters resonated with me.\".split(), 1)\n",
    "\n",
    "]\n",
    "\n",
    "# Printing the data\n",
    "for sentence, label in data:\n",
    "    print(sentence, label)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for sentence, label in data:\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_idx.get(w, 0) for w in sentence]).unsqueeze(0)\n",
    "        outputs = model(sentence)\n",
    "        label = torch.LongTensor([int(label)])\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted label is  tensor([1])\n",
      "None\n",
      "Book Review: The story was captivating and kept me hooked until the end.love\n",
      "Sentiment: Positive\n",
      "\n",
      "the predicted label is  tensor([0])\n",
      "None\n",
      "Book Review: I found the characters shallow and the plot predictable.\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in book_samples:\n",
    "    input_tensor = torch.tensor([word_to_idx[w] if w in word_to_idx else 0 for w in sample[0]], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    outputs = model(input_tensor)\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    print(print(\"the predicted label is \", predicted_label))\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(sample[0])}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for text\n",
    "\n",
    "rnns are suitable of text classification they can read text as humans one word at a time allowing to capture context and order of words\n",
    "\n",
    "sometimes the text may contain complex sentences with diffrent sentiments, lstms excels in them\n",
    "\n",
    "GRU can quickly recognize spammy patterns without needing the full text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x) \n",
    "        # the first represent the output of the lstm at each step\n",
    "        # used in seq2seq and the second is the cell state\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(outputs, y_test_seq)\n",
    "precision_score = precision(outputs, y_test_seq)\n",
    "recall_score = recall(outputs, y_test_seq)\n",
    "f1_score = f1(outputs, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_test, y_pred_lstm)\n",
    "precision_1 = precision(y_test, y_pred_lstm)\n",
    "recall_1 = recall(y_test, y_pred_lstm)\n",
    "f1_1 = f1(y_test, y_pred_lstm)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n",
    "\n",
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_test, y_pred_gru)\n",
    "precision_2 = precision(y_test, y_pred_gru)\n",
    "recall_2 = recall(y_test, y_pred_gru)\n",
    "f1_2 = f1(y_test, y_pred_gru)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
