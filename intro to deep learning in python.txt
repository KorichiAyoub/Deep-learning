forward propagation : passing data through the layers to make predictions 
activation function allow the model to caputure non linearity
- the last layers captures the most high level interaction 
-  gradient descent : 
 - initialize params 
 - compute gradient 
 - update params θ = θ - α * ∇J(θ)
 - repeat if alpha is too big we risk to diverge 

- in deep learing 
 - calculate the slope of loss function multiply by the the feeding value
 - calulate the slope of the activation function then multiply everything this is the slope 


- backpropagation : allows gradient descent to update all weights in neural network by getting gradients for all weights 
 - we go back one layer at a time and then we adjust weights based on the node value slope of loss and slope of activation
 - we updates those layer weights then we move on to the previous layer and we do the same


creating a model with keras 
 - specify the architecture 
 - compile the model 
 - fit the model 
 - predict


u can use np.loadtxt('predictors_data.csv',delimiter=',') to laod data as numpy array since tf is optimized for np arrays 

- in classification we use categorical_crossentropy as the loss func it's the same as the logloss
- u can convert the target to one hot encoding with to_categorical(data['shot_result']) //from tensorflow.keras.utils import to ...


//saving and using a model 
from tensorflow.keras.models import load_model
model.save('model1.h5')
my_model=load_model('model1.h5')
predictions = my_model.predict(data)
probabilty_true = predictions[:,1]


- Early stopping : sometimes when traing the accuracy stops improving at some level so it would be vetter to stop training 
at that level to gain time 
from tensorflow.keras.callbacks import EarlyStopping 
early_stopping_monitor = EarlyStopping(patience=2)// when the accuracy didnt get better for 2 epochs it will stop
model.fit(predictors,target,validation_split =0.3,epochs=20,callbacks=[early_stopping_monitor]) 


when creating a model : usually model with many hidden layers and lots of nodes performs better but sometimes that may 
lead to overfitting 
so to avoid that 
- start with small network 
- increase capacity until the model is no longer imporving (either adding layers or more nodes)
- start decreasing the capacity unti it's no longer imporving 
- done