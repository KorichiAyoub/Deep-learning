a tensor is a generalisation of a vector and matrices
import tensorflow as tf 
tf.ones((2,2,2)) a 3d array matrix of matrices :)
b=tf.constant([1,3,4])
tf.ones_like(give it array and will return array of ones with same shape)
tf.fill([3,3],7)
// to define a variable 
a0=tf.variable([1,2,3],dtype=tf.int16)
tf.multiply(a0,b) or matmul() for matrix mul
u can also convert it to numpy array with a0.numpy()
reduce_sum(A) //sums over all dims
reduce_sum(A,i) //sums over dimension i 
gradient() computes the slope of a function 
to find an optimum u can look for gradient = 0 if the change in gradient > 0 then it's min else max
reshape() reshapres a tensor
random() populates tensor with enteries drawn from a proba distribution

# to calculate the gradient we can use this 
x = tf.Variable(2)
with tf.GradientTape() as tape:
	y= x**2

gradient = tape.gradient(y,x)
print(gradient.numpy()) # result is 4

# reshaping usefull for image processing 
gray  = tf.random.uniform([2,2],maxval=255,dtype='int32')

gray = tf.reshape(gray,[2*2,1]) to make a (4,1) shape  
#another ex 
color  = tf.random.uniform([2,2,3],maxval=255,dtype='int32')
tf.reshape(color,[2*2,3])

# for image classification u have to encode the image as a tensor then work on it 
- when laoding data with pd.read_csv u can work with that data but it is prefirable if u transform it to numpy array cuz
tf is optimized when working with numpy arrays 

- to convert the type in numpy u can 
 price = np.array(df['price'],np.float32) or use tf.cast(df['price'],tf.float32)

loss function 
can use MSE MAE Huber
categorical_crossentropy : for multi classification
binary_crossentropy : for binary classfication

loss = tf.keras.losses.mse(targets,predictions)
def linear_regression(intercept,slope = slope,features = features):
	return intercept + slope*features
def loss(intercept,slope,features=features,targets=targets):
	predictions= linear_regression(intercept,slope)
	return tf.keras.mse(targets,predictions)


and optimizer is used to train the model to reduce the error 
like keras.optimizers.Adam(0.5)//learning rate=0.5
use opt.optimize(lambda: loss_fuction(intercept,slope),var_list=[intercept,slope]) in a loop to minimize the error and note 
that u put only the params to optimize in the list 

#batch training 
is splitting the data into equal batches and train the models on it sequentially
for batch in read_csv(the file,chunksize=100):
	the do the training on each batch

when using batch training 
 - there will be multiple updates per epoch
 - requires division of dataset 
 - no limit on dataset size


the idea behind layers : there is 3 types of layeres input layer,dense(hidden) and output
inputs = tf.constant([[1,3]])
weights =tf.Variable([[-0.05],[-0.01]])
bias = tf.Variable([0.5]) //plays the same role as intercept in the linear regression 
product = tf.matmul(inputs,weights)
dense1 = tf.keras.activations.sigmoid(product +bias) 	
product2 = matmul(dense1,weights2)
prediction = keras.activations.sigmoid(product2 +bias2)

in keras :
inputs = tf.constant(data,tf.float32)
dense1 = tf.keras.layers.Dense(10,activation='sigmoid')(inputs) //by default it includes a bias
#we can add a dense layer
dense2 = tf.keras.layers.Dense(5,activation='sigmoid')(dense1)
output = tf.kears.layers.Dense(1,activation='sigmoid')(dense2)

activation functions :
sigmoid used for binary classification
relu preffered over sigmoid and tanh since it avoides gradient vanishing 
softmax used for multi classification returns values btw 0-1 the sum of them is 1 (probabilites)


optimizers : all take a param learning_rate 
SGD :better in noisy and small datasets
Adam : widely used as it's adaptive and more robust espicilly with large datasets ,momentum reffer as beta
RMSprop: can apply diffrent learning rates to each feature,has momentum parameter



the momentum gradient controlls the influence of the previous update on the curent update , the momentum also helps the 
opt to break through the local min
v = beta * v + (1 - beta) * g
w = w - learning_rate * v

given weight w and its gradient g
v is the moving average of gradients.
beta is the momentum parameter, typically a value between 0 and 1.

 
- initializing the weights : u can use tf.Variable(tf.random.normal([500,500]))
or when using Dense function it has deafault initializer and u can change it by providing the kernel_initializer param 
either to he_normal or glorot_normal ,etc

- to avoid overtting u can use dropout which deletes some arrows going out from some nodes so that the model wont be that 
complex(meaning it sets the value of the weight to 0)
dropout1=tf.keras.Dropout(0.25)(dense2) // this will drop 25% of the nodes 

#the steps to train a model

model = keras.Sequential() // we can put the hidden layers inside or add them with .add func
model.add(keras.layers.Dense(64,activation='relu',input_shape(28*28,))//we usualy specify the shape for first layers only
// the sybsequent layers can infer the shape from the output of previous layer

model.compile('adam',loss='categorical_crossentropy')//chosing th optimiser and the loss func we can add metrics=['accuracy']
so that during the training it will display the accuracy score
model.fit()  # to train the model

-required arguments 
features and labels
-optional  
epochs:how many times we train the model 
batch_size: the number of examples in each batch (an epochs can have many batches)
validation: this one is used during the training while the test used after the training 
validation_split=0.3 the ammount of data to use for validation

model.evaluate(test) //to evaluate the test data u 
#u can get a summary avout the model with model.summary()


u can merge two models with 
merged = tf.keras.add([model1_ouput,model2_output])
model = tf.keras.Model(inputs=[model1_inp,model2_inp],outputs = merged)
model.compile..


#tensorflow apis : high level submodule that enforces best practices allows faster deployment

- low level tf APIs : python
- mid level tf APIs : layers,datasets,metrics
-high level : estimators

- how to use them 
  - define feature columns : this specifies how the model should interpret the data not the data
  - load and transform the data
  - define an estimator
  - apply the train operation 

size=tf.feature_column.numerical_column('size')
rooms=tf.feature_column.categorical_column_with_vocabulary_list('rooms',[1,2,3])
feature_list = [size,rooms]

model= tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10,6,6,3])//this one is for regrression
model= tf.estimator.DNNClassifier(feature_columns=feature_list,hidden_units=[10,6,6],n_classes=4)//this one for classification

there is other estimators like BoostedTreesRegressor(feature_columns=feature_list, n_batches_per_layer=1, n_trees=100) 
b
# the hidden units specifies the number of layers in each node
model.train(features,labels,steps=20) // steps means rhe number of batches not the size of batch


